---
format: revealjs
self-contained: true
---

# Chapter 1

MathStat Preliminaries

## 1.1 Wait. Where are we going?

## Exponential pdf

If $X \sim \text{Exponential}(\lambda)$, then $X$ has the pdf
$$
f(x) = \lambda \exp(-\lambda x) I_{[0,\infty]}(x).
$$

## Exponential pdf visualized

```{r}
#| echo: false
x <- seq(0, 4, len = 1001)
plot(x, exp(-x), xlab = "x", ylab = "density", type = "l")
```

## Sample

A sample of $n$ realization of $X$, denoted $X_1, X_2, \ldots, X_n$, can be used to approximate the distribution of $X$.

## Histogram for 10 realizations

```{r}
#| fig-cap: A histogram of 10 realizations from an Exponential(15.2).
set.seed(1)
x <- seq(0, 3, len = 1000)
hist(rexp(10, rate = 15.2), xlab = "x", probability = TRUE, main = "", ylim = c(0, 15.2))
lines(x, dexp(x, rate = 15.2))
```

## Histogram for 1000 realizations

```{r}
#| fig-cap: A histogram of 1000 realizations from an Exponential(15.2).
set.seed(2)
hist(rexp(1000, rate = 15.2), xlab = "x", probability = TRUE, main = "", ylim = c(0, 15.2), breaks = 30)
lines(x, dexp(x, rate = 15.2))
```

# 1.1.1 A very special trick

## Approach

We can avoid doing actual integration if we can manipulate the integrand to look more like a pdf, which must integrate to 1 of the range of the random variable.

Example: Determine
$$
\int_0^{\infty} 3\exp(-2x)\,dx.
$$

## Special trick (cont)

# 1.2 Transformations of Random Variables

# 1.2.1 The discrete case and the binomial distribution

## The binomial pmf

$X \sim \text{Binomial}(n,p)$ has the pmf
$$
f(x) = P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}I_{\{0,1,\ldots,n\}}(x).
$$

Determine the pmf of $Y:=n-X$.

##

##

##

# 1.2.2 The Continuous Case and the Gamma Distribution

## Continous Transformation PDF

Let $X$ be a continuous random variable with pdf $f_X(x)$.
Let $Y = g(X)$, where $g$ is invertible and differentiable. Then the pdf for $Y$ is 
$$
f_Y(y) = f_X\left(g^{-1}(y)\right)\left|\frac{d}{dy}g^{-1}(y)\right|.
$$

## Proof

## Proof (cont)

## Proof (cont)

## Proof (cont)

## Example 1.2.2

A continuous random variable $X\sim\text{Gamma}(\alpha, \beta)$ has pdf
$$
f_X(x) = \frac{1}{\Gamma(\alpha)}\beta^\alpha x^{\alpha - 1}\exp(-\beta x)I_{(0,\infty)}(x).
$$

## Example 1.2.2 (cont)

Let $Y:= cX$ with $c > 0$. Determine the pdf of $Y$. 

## Example 1.2.2 (cont)


## The shape parameter

The $\alpha$ term is the "shape" parameter of the Gamma distribution.

The kernel (non-constant) part of the pdf of the Gamma distribution is 
$$
x^{\alpha-1}\exp(-\beta x)I_{(0,\infty)}(x).
$$

## The shape parameter

The exponential function part is an unscaled exponential density.

- $\exp(-\beta x)$ dominates $x^{\alpha - 1}$ for large $x$.
- For small $x$, the $x^{\alpha-1}$ term controls the shape.

## Shape parameter visualized

```{r}
#| echo: false
#| fig-cap: The $\alpha$ terms controls the shape of the Gamma distribution.
par(mfrow = c(1, 2))
x <- seq(0, 30, len = 101)
plot(x, dgamma(x, shape = 1), type = "l", xlab = "x", ylab = "density", main = expression(paste(alpha, "=1, ", beta, "=1")))
plot(x, dgamma(x, shape = 10), type = "l", xlab = "x", ylab = "density", main = expression(paste(alpha, "=10, ", beta, "=1")))
par(mfrow = c(1, 1))
```

## The Gamma Function

The pdf for the gamma distribution is defined using the **gamma function**, denoted by $\Gamma(\alpha)$.

$$
\Gamma(\alpha) = \int_0^{\infty} x^{\alpha - 1} \exp(-x)\, dx.
$$

## The Gamma Function (cont)

## The Gamma Function (cont)

## The Gamma Function (cont)

# 1.3 Bivariate Transformations

## Bivariate Transformation PDF

Suppose the $X_1$ and $X_2$ are jointly continuous random variables with pdf $f_{X_1, X_2}(x_1, x_2)$. Let $Y_1 = g_1(X_1, X_2)$ and $Y_2 = g_2(X_1, X_2)$. For $h_1$ and $h_2$ differentiable, let 
$$
X_1 = h_1(Y_1, Y_2)\text{ and }X_2 = h_2(Y_1, Y_2).
$$
Then
$$
f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}\left(h_1(y_1, y_2), h_2(y_1, y_2)\right)|J|,
$$
where $|J|$ is the absolute value of the Jacobian.

## Bivariate transformation pdf continued

The absolute value of the Jacobian is
$$
J = 
\text{det}\left[
\begin{matrix}
\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\
\frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2}
\end{matrix}
\right].
$$

## Example 1.3.1

Let $X_1, X_2 \stackrel{i.i.d.}{\sim}\text{Gamma}(\alpha, \beta)$

Determine the pdf of 
$$
Y = \frac{X_1}{X_1 + X_2}.
$$

## Example 1.3.1 (cont)

## Example 1.3.1 (cont)

## Example 1.3.1 (cont)

## Example 1.3.1 (cont)

## The Beta Distribution

The continuous random variable $X\sim \text{Beta}(a,b)$ had pdf
$$
f(x) = \frac{1}{\mathcal{B}(a,b)}x^{a-1}(1-x)^{b-1}I_{(0,1)}(x),
$$
for $a,b>0$, where 

$$
\mathcal{B}(a,b)=\int_0^1x^{a-1}(1-x)^{b-1}\,dx
$$
denotes the **beta function**, which normalizes the kernel of the Beta distribution. 

## The Beta Distribution (cont)

The Beta distribution is a flexible distribution for modeling a random variable between 0 and 1.

Note: The Uniform(0, 1) distribution is a special case of the Beta distribution with 

## The Beta Distribution (cont)

```{r}
x <- seq(0, 1, len = 1001)
par(mfrow = c(2, 3))
plot(x, dbeta(x, 0.5, 0.5), ylab = "density", type = "l",
     main = "a = 0.5, b = 0.5")
plot(x, dbeta(x, 1, 1), ylab = "density", type = "l",
     main = "a = 1, b = 1")
plot(x, dbeta(x, 9, 3), ylab = "density", type = "l",
     main = "a = 9, b = 3")
plot(x, dbeta(x, 3, 9), ylab = "density", type = "l",
     main = "a = 3, b = 9")
plot(x, dbeta(x, 4, 4), ylab = "density", type = "l",
     main = "a = 4, b = 4")
plot(x, dbeta(x, 10, 200), ylab = "density", type = "l",
     main = "a = 10, b = 200")
par(mfrow = c(1, 1))
```

## The Beta Function

$$
\mathcal{B}(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}.
$$

Proof:

## The Beta Function (cont)

# 1.4 Minimums and Maximums

## Order statistics notation

Let $X_1, X_2, \ldots, X_n$ denote a sample of observations. The order statistics are denoted as:

- $X_{(1)} = \min(X_1, X_2, \ldots, X_n)$.
- $X_{(2)} = \text{2nd smallest of }X_1, X_2, \ldots, X_n$.
- $X_{(n)} = \max(X_1, X_2, \ldots X_n)$.

# 1.4.2 The distribution of a minimum by example

## Example 1.4.1

Let $X_1, X_2, \ldots, X_n \stackrel{i.i.d.}{\sim}\text{Exponential}(\lambda)$. Determine the distribution of $X_{(1)}$.

## Example 1.4.1 (cont)

## Example 1.4.1 (cont)

## Example 1.4.1 (cont)

## Example 1.4.1 (cont)

# 1.4.3 The general distribution of minimums and maximums

## Distribution of the minimum

Let $X_1, X_2, \ldots, X_n$ be a random sample from a continuous distribution. Determine the distribution of $X_{(1)}$.

## Distribution of the minimum (cont)

## Distribution of the minimum (cont)

## Distribution of the minimum (cont)

## PDF for a minimum

Let $X_1, X_2, \ldots, X_n$ be a random sample from a continuous distribution with pdf $f$ and cdf $X$. 

The pdf for $X_{(1)}$ is
$$
f_{X_{(1)}} = n [1 - F(x)]^{n-1} f(x).
$$

## Distribution of the maximum

Let $X_1, X_2, \ldots, X_n$ be a random sample from a continuous distribution. Determine the distribution of $X_{(n)}$.

## Distribution of the maximum (cont)

## Distribution of the maximum (cont)

## Distribution of the maximum (cont)

## PDF for a maximum

Let $X_1, X_2, \ldots, X_n$ be a random sample from a continuous distribution with pdf $f$ and cdf $X$. 

The pdf for $X_{(n)}$ is
$$
f_{X_{(n)}} = n [F(x)]^{n-1} f(x).
$$

# 1.5 Moment Generating Functions (mgfs)

## Definition of the mgf

The moment generating function (mgf) of a random variable $X$ is
$$
M_X(t) = E[e^{tX}].
$$

## Uses of mgfs

Mgfs are often useful in two settings:

1. Determining the distribution of a transformation of 1 or more random variables (particularly the sum of $n$ i.i.d. random variables.)
2. Determine the "moments" of a random variable, i.e., $E(X), E(X^2), E(X^3), \ldots$.

# 1.5.1 The expectation of a function $X$

## Example 1.5.1

Suppose the random variable $X$ has the following distribution.

| $x$ | -1 | 0 | 1
| :--- | :--- | :--- | :--- |
| $f(x)$ | 4/12 | 3/12 | 5/12

Determine $E(X^2)$.

## Example 1.5.1 (cont)

## Example 1.5.1 (cont)

## Law of the Unconscious Statistician

If $X$ is a discrete random variable, then 
$$
E[g(X)] = \sum_x g(x) f_X(x),
$$
where $f_X(x)$ is the pmf of $X$.


If $X$ is a continuous random variable, then 
$$
E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x)\,dx,
$$
where $f_X(x)$ is the pdf of $X$.

## Proof (specific case)

## Proof (specific case)

## Proof (specific case)

## Proof (specific case)

# 1.5.2 Finding mgfs and the Poisson distribution

## The Poisson distribution

A discrete random variable $X\sim\text{Poisson}(\lambda)$ if it has the pmf
$$
f_X(x) = \frac{e^{-\lambda}\lambda^x}{x!}I_{\{0,1,2,\ldots\}}(x).
$$

## Poisson interpretation

A Poisson random variable can be thought of as the number of arrivals observed in a fixed amount of time where:

1. The mean time between arrivals is constant.
2. The number of arrivals in non-overlapping time periods are independent. 

## Example 1.5.2 

Determine the mgf for the Poisson distribution

## Example 1.5.2 (cont)

## Example 1.5.2 (cont)

## Example 1.5.3

Suppose that $X\sim\text{Exponential}(\lambda)$. Determine the mgf of $X$.

## Example 1.5.3 (cont)

## Example 1.5.3 (cont)

## Example 1.5.4

Suppose that $X\sim N(\mu, \sigma^2)$ with pdf
$$
f_X(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2\sigma^2}(x-\mu)^2\right].
$$
Determine the mgf of $X$.

## Example 1.5.4 (cont)

## Example 1.5.4 (cont)

## Example 1.5.4 (cont)

# 1.5.3 Finding moments

## Defining moments

The **moments** of a random variable $X$ are expectations of the form $E(X^k)$ for $k=1,2,3,\ldots$.

## Computing moments

The $k$th moment of a random variable $X$ can be computed as $M^{(k)}_X(0)$, where $M^{(k)}_X(0)$ is the $k$th derivative of the mgf of $X$ evaluated at zero.

## Mgf moment proof

## Mgf moment proof (cont) 

## Mgf moment proof (cont)

## Example 1.5.5

Determine the mean and variance of the $\text{Poisson}(\lambda)$ distribution.

## Example 1.5.5 (cont)

## Example 1.5.5 (cont)

# 1.5.4 The mgf uniquely determines a distribution

## The DNA of the mgf

The mgf of a random variable uniquely determines its distribution.

Example: If we find the mgf of a random variable $X$ is $\exp[15(e^t - 1)]$, then $X\sim\text{Poisson}(15)$ since it matches the mgf of a Poisson random variable with $\lambda = 15$.

# 1.5.6 Sums of iid random variables

## Mgf of sum of independent r.v.s

Let $X_1, X_2, \ldots, X_n$ be independent random variables. The mgf of
$$
Y:=\sum_{i=1}^n X_i
$$
is 
$$
M_Y(t) = \prod_{i=1}^n M_{X_i}(t).
$$

## Proof

## Proof (cont)

## Proof (cont)

## Mgf of sum of i.i.d. r.v.s

Let $X_1, X_2, \ldots, X_n\stackrel{i.i.d.}{\sim}F$. The mgf of
$$
Y:=\sum_{i=1}^n X_i
$$
is 
$$
M_Y(t) = [M_{X}(t)]^n.
$$

## Example 1.5.6

Suppose that $X_1,X_2,\ldots,X_n\stackrel{i.i.d.}{\sim}\text{Poisson}(\lambda)$. Determine the distribution of $Y:=\sum_{i=1}^n X_i$.

## Example 1.5.6 (cont)

## Example 1.5.6 (cont)

## Example 1.5.7

Suppose that $X_1,X_2,\ldots,X_n\stackrel{i.i.d.}{\sim}\text{Exponential}(\lambda)$. Determine the distribution of $Y:=\sum_{i=1}^n X_i$.

## Example 1.5.7 (cont)

## Example 1.5.7 (cont)

## Example 1.5.8

Suppose that $X_1,X_2,\ldots,X_n$ be independent random variables with $X_i\sim\text{Poisson}(\lambda_i)$. Determine the distribution of $Y:=\sum_{i=1}^n X_i$.

## Example 1.5.8 (cont)

## Example 1.5.8 (cont)

## Example 1.5.8 (cont)

# 1.6 General Order Statistics

# 1.6.2 The joint distribution of the minimum and maximum

## Min/Max cdf

For an i.i.d. sample of continuous random variables $X_1, X_2, \ldots, X_n$ with cdf $F$, the joint cdf of $(X_{(1)}, X_{(n)})$ is
$$
F_{X_{(1)}, X_{(n)}}(x, y) = [F(y)]^n - [F(y) - F(x)]^n,
$$
for $x<y$. 

## Proof of min/max cdf

## Proof of min/max cdf (cont)

## Proof of min/max cdf (cont)

## Min/Max pdf

## Example 1.6.1

Determine the joint pdf of $(X_{(1)}, X_{(15)})$ when $X_1, X_2, \ldots, X_15 \stackrel{i.i.d.}{\sim}\text{Uniform}(0, 1)$.

## Example 1.6.1 (cont)

# 1.6.4 The distribution of $X_{(i)}$.

## CDF of $X_{(i)}$

For an i.i.d. sample of continuous random variables $X_1, X_2, \ldots, X_n$ with cdf $F$, the cdf of $X_{(i)}$ is
$$
F_{X_{(i)}}(x) = P(X_{(i)}\leq x) = \sum_{j=i}^n \binom{n}{j}(1-F(x))^{n-j}F(x)^j.
$$

## Proof

## Proof (cont)


## The PDF of $X_{(i)}$

For an i.i.d. sample of continuous random variables $X_1, X_2, \ldots, X_n$ with cdf $F$, the pdf of $X_{(i)}$ is
$$
f_{X_{(i)}}(x) = \frac{n!}{(i-1)!(n-i)!}F(x)^{i-1}(1-F(x))^{n-k}f(x).
$$

## Proof

## Proof (cont)

## Proof (cont)

# 1.6.5 The joint distribution of $X_{(i)}$ and $X_{(j)}$

## The joint pdf of $X_{(i)}$ and $X_{(j)}$

For an i.i.d. sample of continuous random variables $X_1, X_2, \ldots, X_n$ with cdf $F$, the joint pdf of $(X_{(i)},X_{(i)})$ is 
\begin{align}
&f_{X_{(i)},X_{(j)}}(x_i, x_j)\\
&= \frac{n!}{(i-1)!(j-i-1)!(n-j)!}F(x_i)^{i-1}f(x_i)\\
&\quad\times[F(x_j)-F(x_i)]^{j-i-1}f(x_j)[1-F(x_j)]^{n-j}I_{(-\infty,x_j)}(x_i).
\end{align}

